/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package io.confluent.csta.examples.transactions.consumer.transactional;

import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.time.Duration;
import java.util.List;
import java.util.Properties;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.errors.RecordDeserializationException;
import org.apache.kafka.common.header.Header;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.slf4j.event.Level;

public class TransactionalConsumer {

    private static final Logger LOGGER = LoggerFactory.getLogger(TransactionalConsumer.class);
    private static final Duration POLL_TIMEOUT = Duration.ofSeconds(5);

    public static Properties loadConfig(final String configFile) throws IOException {
        if (!Files.exists(Paths.get(configFile))) {
            throw new IOException(configFile + " not found.");
        }
        final Properties cfg = new Properties();
        try (InputStream inputStream = new FileInputStream(configFile)) {
            cfg.load(inputStream);
        }
        return cfg;
    }

    public static void main(String[] args) {
        if (args.length != 1) {
            System.err.println("Usage: java <jar file> <property file>");
            System.exit(1);
        }
        LOGGER.info("Starting consumer");
        try {
            final Properties config = loadConfig(args[0]);
            /*
             * config.put("client.id", "TransactionalProducer"); // Make sure /etc/hosts contains
             * aliases for localhost named kafka1, kafka2, kafka3 config.put("bootstrap.servers",
             * "kafka1:9091,kafka2:9092,kafka3:9093");
             */
            config.put("key.deserializer",
                    "org.apache.kafka.common.serialization.StringDeserializer");
            config.put("value.deserializer",
                    "org.apache.kafka.common.serialization.StringDeserializer");
            config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
            config.put("enable.auto.commit", "false");
            config.put("isolation.level", "read_committed");
            String topic = config.getProperty("topic");
            config.remove("topic");
            try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(config)) {
                // Subscribe to our topic
                LOGGER.error ("Subscribing to topic "+topic);
                consumer.subscribe(List.of(topic));
                // Seek to beginning
                while(consumer.assignment().isEmpty()) { consumer.poll(Duration.ofMillis(10)); }
                consumer.seekToBeginning(consumer.assignment());
                // noinspection InfiniteLoopStatement
                while (true) {
                    try {
                        final var records = consumer.poll(POLL_TIMEOUT);
                        int count = records.count();
                        if (count != 0) {
                            LOGGER.info("Poll return {} records", count);
                        }
                        for (var record : records) {
                            LOGGER.info("Fetch record value={}", 
                                    record.value());
                            System.out.println(record.value());
                            for (Header header: record.headers()) {
                                System.out.println(header.toString());
                            }
                            System.out.println("");
                        }
                    } catch (RecordDeserializationException re) {
                        long offset = re.offset();
                        Throwable t = re.getCause();
                        LOGGER.error("Failed to consumer at partition={} offset={}",
                                re.topicPartition().partition(), offset, t);
                        LOGGER.info("Skipping offset={}", offset);
                        consumer.seek(re.topicPartition(), offset + 1);
                    } catch (Exception e) {
                        LOGGER.error("Failed to consumer", e);
                    }
                }
            } finally {
                LOGGER.info("Closing consumer");
            }
        } catch (IOException e) {
            System.err.println("An exception occurred while load properties file: " + e);
            System.exit(1);
        }

    }
}
